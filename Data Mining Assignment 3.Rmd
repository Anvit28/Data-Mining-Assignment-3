---
title: "Data Mining Assignment 3"
author: ""
date: ""
output:
  md_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# ECO 395M: Exercises 2  
# Group Members - Alina Khindanova, Anvit Sachdev, Shreya Kamble  
## Problem 1: What Causes What?
1- Why can’t I just get data from a few different cities and run the regression of “Crime” on  “Police” to understand how more cops in the streets affect crime? (“Crime” refers to some 
measure of crime rate and “Police” measures the number of cops in a city.)  
Answer:-  
The reason we have an endogeneity issue in our regression (Crime on Police) is that the police variable is correlated with the error term, meaning that high crime rates lead to more police hires. This indicates that our results will be unreliable and biased. There are a few ways we can avoid this problem, such as finding and including missing variables, using a proxy variable, using a fixed effect estimator with panel data, or using an IV to replace the endogenous variable with the predicted value that has only exogenous shocks.  

2- How were the researchers from UPenn able to isolate this effect? Briefly describe their approach and discuss their result in the “Table 2” below, from the researchers’ paper.  
Answer:-  
The researchers wanted to see if having more police officers on the street in Washington DC would reduce crime rates. However, they had a problem with their analysis because the number of cops might be related to crime rates in a way that could cause biased results. To deal with this, they used a variable called “Terror Alert” as a substitute for the number of cops, since it is an unrelated variable that also affects crime rates. They found that when the Terror Alert level was raised, crime rates went down. But also, there was another thing that is worth noticing is that the variable might not be a perfect substitute, so they included another variable called metro ridership to check if that could be causing any changes in crime rates. They found that even after accounting for metro ridership, raising the number of cops still led to a decrease in crime rates by 6.1. So, in conclusion the study suggests that increasing the number of cops on the street could reduce crime rates in Washington DC.  

3- Why did they have to control for Metro ridership? What was that trying to capture?  
Answer:-  
The data on police and crime cannot determine if having more police causes less crime or if more crime leads to the hiring of more police officers. In fact, if we look at different cities, we might see a correlation between more police and more crime, because when crime goes up, mayors may hire more police officers. It would be great to conduct an experiment where we randomly place police officers in different parts of a city on different days to see how it affects crime, but that's not feasible in reality. The researchers at UPENN found a natural experiment by studying crime in DC during high alert days for potential terrorist attacks. By law, the mayor has to put more police officers on the 
streets during these days, which creates an experimental setting. The researchers found that, when controlling for subway ridership, high alert days had a lower crime rate. This is because, if people stay indoors during high alert days, there are fewer opportunities for crime to occur. The study shows that having more police officers on the streets during high alert days can have a negative impact on crime. We can't definitively prove that having more police officers leads to less crime because there could be other factors at play. For example, if criminals are afraid of potential terrorist attacks and stay home during high alert days, there would be less crime, but it wouldn't be because of the 
increased police presence. However, this explanation is unlikely, and the study's results strongly suggest that having more police officers on the streets can reduce crime.  
4- Answer:-  
The researchers looked at whether the increase in police presence during high alert days had the same effect on crime in all areas of town. They found that the increase in police presence only seemed to make a difference in District 1, which is where most of the potential targets for terrorism are located. In the other districts, there was still a small decrease in crime, but it was not statistically significant, meaning it could still be due to chance. This suggests that the presence of more police officers may be effective in preventing crime in certain areas, but not necessarily everywhere.  
## Problem 2: Tree modeling: dengue cases  
Dataset: dengue.csv   
```{r 2.loading libraries, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(dplyr)
library(caret)
```
```{r 2.reading data, echo=FALSE, message=FALSE, warning=FALSE}
dengue = read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/dengue.csv")
#print(dengue)
```
The features of the dataset that I have used in the models are: city, season, precipitation_amt,air_temp,avg_temp_k,dew_point_temp_k,specific_humidity and tdtr_k.  
```{r 2.removing null value observations from the dataset, echo=FALSE, message=FALSE, warning=FALSE}
dengue=na.omit(dengue)
```
I have used one-hot encoding to deal with dummy variables of the data set.  
```{r 2.one-hot encoding of the dummy variables, echo=FALSE, message=FALSE, warning=FALSE}
dummy = dummyVars(" ~ .", data=dengue)
dengue = data.frame(predict(dummy, newdata = dengue))
#dengue
```
Since there are also many observations in the data set with zero dengue cases so using log dengue cases does not make sense. So we are predicting dengue cases.  
```{r 2.loading library splitTools, echo=FALSE, message=FALSE, warning=FALSE}
library(splitTools)
```
Next, we split the data into training and test data. The training data comprises of 80% of overall data, and test data comprises of remaining 20% of the data.  
```{r 2.dividing the dataset into training and test set, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(3451)
inds <- partition(dengue$total_cases, p = c(train = 0.8, test = 0.2))
#str(inds)
train <- dengue[inds$train, ]
test <- dengue[inds$test, ]
#train
```
```{r 2.defining the function that calculates root mean squared error, echo=FALSE, message=FALSE, warning=FALSE}
rmse <- function(y, pred) {
  sqrt(mean((y - pred)^2))
}
```
I have performed k-fold (k=5) cross-validation on training set for tuning hyper parameters of the models.  
```{r 2.creating folds for cross-validation of training set, echo=FALSE, message=FALSE, warning=FALSE}
# Get stratified cross-validation in-sample indices
folds <- create_folds(train$total_cases, k = 5)
rpart.grid <- expand.grid(.cp=0.2)
```
We obtain the following RMSE values for CART, Random Forest and Gradient-Boosted Trees:-  
```{r 2.CART 1, echo=FALSE, message=FALSE, warning=FALSE}
# loading CART libraries
library(rpart)
library("rpart.plot")
library(caret)
library(mlr)
```
```{r 2.CART 2, echo=FALSE, message=FALSE, warning=FALSE}
# using k-fold (k=5) cross-validation on training data to find the best maxdepth of CART.
maxdepths=1:30
for (i in maxdepths) {
  cv_mtry <- numeric()
  for (fold in folds) {
    fit <- rpart(total_cases ~ cityiq + citysj + seasonfall + seasonspring + seasonsummer + seasonwinter + precipitation_amt+air_temp_k+avg_temp_k+dew_point_temp_k+specific_humidity+tdtr_k,method="anova", data = train[fold, ], maxdepth=i)
    cv_mtry <- c(cv_mtry, 
                 rmse(train[-fold, "total_cases"], predict(fit,train[-fold,])))
  }
  maxdepths[i] <- mean(cv_mtry)
}
#maxdepths
```
```{r 2.CART 3, echo=FALSE, message=FALSE, warning=FALSE}
# finding the best hyper parameter (that gave the least RMSE)
best_depth <- which.min(maxdepths)
# checking RMSE of the CART model with the best hyper parameter on test data
final_fit <- rpart(total_cases ~ cityiq + citysj + seasonfall + seasonspring + seasonsummer + seasonwinter + precipitation_amt+air_temp_k+avg_temp_k+dew_point_temp_k+specific_humidity+tdtr_k, data = train, method="anova",maxdepth=best_depth)
print(paste0("The RMSE of CART model on test data is: ", rmse(test$total_cases, predict(final_fit, test))))
```
```{r 2.Random Forest 1, echo=FALSE, message=FALSE, warning=FALSE}
# loading the ranger library
library(ranger)
```
```{r 2.Random Forest 2, echo=FALSE, message=FALSE, warning=FALSE}
# using k-fold (k=5) cross-validation on training data to find the best mtry (the number of variables to randomly sample as candidates at each split) of Random Forest.
valid_mtry <- numeric(12)
for (i in seq_along(valid_mtry)) {
  cv_mtry <- numeric()
  for (fold in folds) {
    fit <- ranger(total_cases ~ cityiq + citysj + seasonfall + seasonspring + seasonsummer + seasonwinter + precipitation_amt+air_temp_k+avg_temp_k+dew_point_temp_k+specific_humidity+tdtr_k, data = train[fold, ], mtry = i)
    cv_mtry <- c(cv_mtry, 
                 rmse(train[-fold, "total_cases"], predict(fit, train[-fold, ])$predictions))
  }
  valid_mtry[i] <- mean(cv_mtry)
}
#valid_mtry
```
```{r 2.Random Forest 3, echo=FALSE, message=FALSE, warning=FALSE}
# finding the best hyper parameter (that gave the least RMSE)
best_mtry <- which.min(valid_mtry)
# checking RMSE of the random forest model with the best hyper parameter on test data
final_fit <- ranger(total_cases ~ cityiq + citysj + seasonfall + seasonspring + seasonsummer + seasonwinter + precipitation_amt+air_temp_k+avg_temp_k+dew_point_temp_k+specific_humidity+tdtr_k, data = train, mtry = best_mtry)
print(paste0("The RMSE of Random Forest model on test data is: ",rmse(test$total_cases, predict(final_fit, test)$predictions)))
```
```{r 2.Gradient Boosted Trees 1, echo=FALSE, message=FALSE, warning=FALSE}
# loading library gbm
library(gbm)
```
```{r 2.Gradient Boosted Trees 2, echo=FALSE, message=FALSE, warning=FALSE}
# defining a grid that has different hyper parameters of the Gradient Boosted Tree
hyper_grid <- expand.grid(
  shrinkage = c(.01, .05),
  interaction.depth = c(3, 5),
  n.minobsinnode = c(5, 7),
  bag.fraction = c(.65, 1), 
  optimal_trees = 0,               # a place to dump results
  RMSE = 0                     # a place to dump results
)
# using k-fold (k=5) cross-validation on training data to find the best hyper parameters of the Gradient Boosted Tree
for(i in 1:nrow(hyper_grid)) {
  for (fold in folds) {
    gbm.tune <- gbm(
    formula = total_cases ~ cityiq + citysj + seasonfall + seasonspring + seasonsummer + seasonwinter + precipitation_amt+air_temp_k+avg_temp_k+dew_point_temp_k+specific_humidity+tdtr_k,
    distribution = "gaussian",
    data = train[fold,],
    n.trees = 5000,
    interaction.depth = hyper_grid$interaction.depth[i],
    shrinkage = hyper_grid$shrinkage[i],
    n.minobsinnode = hyper_grid$n.minobsinnode[i],
    bag.fraction = hyper_grid$bag.fraction[i],
    train.fraction = .75,
    n.cores = NULL, # will use all cores by default
    verbose = FALSE
  )
    cv_mtry <- c(cv_mtry, 
                 rmse(train[-fold, "total_cases"], predict(gbm.tune, train[-fold, ])))
  }
  hyper_grid$RMSE=mean(cv_mtry)
}
hyper_grid$RMSE
```
```{r 2.Gradient Boosted Trees 3, echo=FALSE, message=FALSE, warning=FALSE}
# finding the best hyper parameter (that gave the least RMSE)
best_hyperparameter <- which.min(hyper_grid$RMSE)
# checking RMSE of the Gradient Boosted Tree model with the best hyper parameter on test data
final_fit <- gbm(
    formula = total_cases ~ cityiq + citysj + seasonfall + seasonspring + seasonsummer + seasonwinter + precipitation_amt+air_temp_k+avg_temp_k+dew_point_temp_k+specific_humidity+tdtr_k,
    distribution = "gaussian",
    data = train,
    n.trees = 5000,
    interaction.depth = hyper_grid$interaction.depth[best_hyperparameter],
    shrinkage = hyper_grid$shrinkage[best_hyperparameter],
    n.minobsinnode = hyper_grid$n.minobsinnode[best_hyperparameter],
    bag.fraction = hyper_grid$bag.fraction[best_hyperparameter],
    train.fraction = .75,
    n.cores = NULL, # will use all cores by default
    verbose = FALSE
  )
print(paste0("The RMSE of Gradient Boosted Tree model on test data is: ",rmse(test$total_cases, predict(final_fit, test))))
```
We can see that Random Forest performs the best. We will now use this model to make partial dependence plots.  
```{r 2.loading libraries for partial dependence plots, echo=FALSE, message=FALSE, warning=FALSE}
library(pdp)
library(ggplot2)
library(gridExtra)
```
We obtain the following partial dependence plot for specific_humidity:-    
```{r 2.partial dependence plot for specific_humidity, echo=FALSE, message=FALSE, warning=FALSE} 
final_fit <- ranger(total_cases ~ cityiq + citysj + seasonfall + seasonspring + seasonsummer + seasonwinter + precipitation_amt+air_temp_k+avg_temp_k+dew_point_temp_k+specific_humidity+tdtr_k, data = train, mtry = best_mtry)
par <- partial(final_fit, pred.var = c("specific_humidity"), chull = TRUE)
plot_1 <- autoplot(par, contour = TRUE)
grid.arrange(plot_1)
```
We obtain the following partial dependence plot for precipitation_amt:-  
```{r 2.partial dependence plot for precipitation_amt, echo=FALSE, message=FALSE, warning=FALSE}
par <- partial(final_fit, pred.var = c("precipitation_amt"), chull = TRUE)
plot_2 <- autoplot(par, contour = TRUE)
grid.arrange(plot_2)
```
We obtain the following partial dependence plot for avg_temp_k:-  
```{r 2.partial dependence plot for avg_temp_k, echo=FALSE, message=FALSE, warning=FALSE}
par <- partial(final_fit, pred.var = c("avg_temp_k"), chull = TRUE)
plot_3 <- autoplot(par, contour = TRUE)
grid.arrange(plot_3)
```

## Problem 3: Predictive Modelling- Green Certification

```{r , echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
library(tidyverse)
library(lubridate)
library(randomForest)
library(gbm)
library(pdp)
library(modelr)
library(rsample)
library(rpart)
library(rpart.plot)
library(caret)
library(textir)
library(corrplot)
library(gridExtra)
library(GGally)
library(e1071)
library(ggthemes)
library(scales)
library(class) 
library(ggmap)
greenbuildings <- read.csv("C:/Users/shrey/Downloads/greenbuildings.csv")
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
set.seed(485)
greenbuildings$renovated = factor(greenbuildings$renovated)
greenbuildings$class_a = factor(greenbuildings$class_a)
greenbuildings$class_b = factor(greenbuildings$class_b)
greenbuildings$LEED = factor(greenbuildings$LEED)
greenbuildings$Energystar = factor(greenbuildings$Energystar)
greenbuildings$green_rating = factor(greenbuildings$green_rating)
greenbuildings$net = factor(greenbuildings$net)
greenbuildings$amenities = factor(greenbuildings$amenities)

greenbuildings1 = greenbuildings %>%
  mutate(revenue = Rent*leasing_rate)
set.seed(485)
greenbuildings1_split =  initial_split(greenbuildings1, prop=0.8)
greenbuildings1_split_train = training(greenbuildings1_split)
greenbuildings1_split_test  = testing(greenbuildings1_split)


```

I tried out four different machine learning models to see which one would make the best predictions. Three of them were random forests and the other one was a gradient boosting model.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(485)
forest_green = randomForest(revenue ~ . ,
                       data=greenbuildings1_split_train, na.action = na.exclude)

varImpPlot(forest_green)
  rmse_green1 = modelr::rmse(forest_green, greenbuildings1_split_test)
  
  set.seed(485)
  forest_green2 = randomForest(revenue ~ Rent + City_Market_Rent + leasing_rate + Electricity_Costs + size + CS_PropertyID + stories + age + green_rating  ,
                       data=greenbuildings1_split_train, na.action = na.exclude)
  
  rmse_green2 = modelr::rmse(forest_green2, greenbuildings1_split_test)  
  
  
    set.seed(485)
  forest_green3 = randomForest(revenue ~ Rent + City_Market_Rent + leasing_rate + Electricity_Costs + size + CS_PropertyID + stories + age + hd_total07  + total_dd_07 + total_dd_07 + green_rating,
                       data=greenbuildings1_split_train, na.action = na.exclude)
  rmse_green3 = modelr::rmse(forest_green3, greenbuildings1_split_test)  
  
boost_green = gbm(revenue ~ Rent + City_Market_Rent + leasing_rate + Electricity_Costs + size + CS_PropertyID + stories +green_rating, 
             data = greenbuildings1_split_train,
             interaction.depth=4, n.trees=350, shrinkage=.02)
  rmse_green4 = modelr::rmse(boost_green, greenbuildings1_split_test)  
  
  
models_green_summary = data.frame(
RFM1_rmse = rmse_green1,
RFM2_rmse = rmse_green2,
RFM3_rmse = rmse_green3,
Boost_rmse = rmse_green4)
models_green_summary
  yhat_green_gbm = predict(boost_green, greenbuildings1_split_test, n.trees=350)
```
Now I'm going to see how the green rating changes based on the optimal machine learning model that I found earlier. I'm going to do this by checking the partial dependence of the green rating. This means I'll look at how the green rating changes when I change other variables in the model.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
  plot(boost_green, 'green_rating')
  
  p4 = pdp::partial(boost_green, pred.var = 'green_rating', n.trees=350)
p4
```
The goal of this exercise is to figure out how much money commercial rental properties make per square foot each year, and whether having a green certification helps them make more money. To do this, I have mutated a new column to calculate the revenue per square foot per calender year based on the original data. I also made sure that some of the variables were dummy variables.
Then, I split the data into two parts: one to train the computer on how to predict revenue, and another to test how well it did. I used a machine learning technique called random forest to make the prediction. The first model used all the variables, and then I checked which ones were most important so I could make other models and compare them. I wanted to see which model would give the most accurate predictions.

Based on the importance of the variables in the first model, I made two more models to see which one would give the best predictions. Even though the green rating wasn't very important in the model, I still included it to check the real effect using a special algorithm.The second model used 9 variables with different levels of importance, while the third model used 12 variables, including some that were not very important. I also checked the RMSE for each model and compared them to the first model. The second model had a slightly lower RMSE than the first model, which is good, but we want to find the best possible model. So, I tried a different type of machine learning called gradient boosting, using the same variables as the second model. I want to see if this will give even better predictions.

So, I tried different shrinkage rates to see which one works best to predict revenue for rental properties, and found that the gradient boosting model works better than the random forest models.I was able to make a model that is better than the 2nd model by reducing the error rate to 108, while the best random forest model had an error rate of 148. In other words, my new model is better at predicting the revenue per square foot of the commercial rental properties.When I checked the effect of green certification on the revenue, I found out that it doesn't really make a difference whether a property is certified green or not. This means that even if a property has a green certification from LEED or Energystar, it won't necessarily bring in more revenue. I did this by predicting the average revenue for both certified and non-certified properties, and found that the values are almost the same. The graph also confirms this.

## Problem 4: Predictive model building: California housing    
Dataset: CAhousing.csv   
```{r 4.loading libraries, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(dplyr)
library(caret)
library(splitTools)
library(ranger)
```
```{r 4.reading dataset, echo=FALSE, message=FALSE, warning=FALSE}
CAhousing = read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/CAhousing.csv")
#print(CAhousing)
```
Instead of using total rooms and total bedrooms, I am using average rooms per household and average Bedrooms per household (by diving totalRooms and totalBedrooms by households).   
```{r 4.feature engineering, echo=FALSE, message=FALSE, warning=FALSE}
CAhousing=na.omit(CAhousing)
CAhousing$avg_rooms=CAhousing$totalRooms/CAhousing$households
CAhousing$avg_bedrooms=CAhousing$totalBedrooms/CAhousing$households
```
Next, we split the data into training and test data. The training data comprises of 80% of overall data, and test data comprises of remaining 20% of the data. 
```{r 4.partitioning data set into training and test data, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(3451)
inds <- partition(CAhousing$medianHouseValue, p = c(train = 0.8, test = 0.2))
train <- CAhousing[inds$train, ]
test <- CAhousing[inds$test, ]
#train
```
```{r 4.defining the function that calculates root mean squared error, echo=FALSE, message=FALSE, warning=FALSE}
rmse <- function(y, pred) {
  sqrt(mean((y - pred)^2))
}
```
I have performed k-fold (k=5) cross-validation on training set for tuning hyper parameters of the models.  
```{r 4.creating folds for cross-validation of training set, echo=FALSE, message=FALSE, warning=FALSE}
# Get stratified cross-validation in-sample indices
folds <- create_folds(train$medianHouseValue, k = 5)
rpart.grid <- expand.grid(.cp=0.2)
```
I tried CART, Random Forest and Gradient-Boosted Trees to predict the medianHouseValue.  
We obtain the following RMSE values for CART, Random Forest and Gradient-Boosted Trees:-  
```{r 4.Model 1: CART, echo=FALSE, message=FALSE, warning=FALSE}
# loading libraries for CART
library(rpart)
library("rpart.plot")
library(caret)
library(mlr)
```
```{r 4.CART 2, echo=FALSE, message=FALSE, warning=FALSE}
# using k-fold (k=5) cross-validation on training data to find the best maxdepth of CART.
maxdepths=1:30
for (i in maxdepths) {
  cv_mtry <- numeric()
  for (fold in folds) {
    fit <- rpart(medianHouseValue ~ longitude + latitude + housingMedianAge + population + avg_bedrooms + avg_rooms + medianIncome,method="anova", data = train[fold, ], maxdepth=i)
    cv_mtry <- c(cv_mtry, 
                 rmse(train[-fold, "medianHouseValue"], predict(fit,train[fold,])))
  }
  maxdepths[i] <- mean(cv_mtry)
}
#maxdepths
```
```{r 4.CART 3, echo=FALSE, message=FALSE, warning=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
# finding the best hyper parameter (that gave the least RMSE)
best_depth <- which.min(maxdepths)
# checking RMSE of the CART model with the best hyper parameter on test data
final_fit <- rpart(medianHouseValue ~ longitude + latitude + housingMedianAge + population + avg_bedrooms + avg_rooms + medianIncome,method="anova", data = train, maxdepth=best_depth)
print(paste0("The RMSE of CART model on test data is: ", rmse(test$medianHouseValue, predict(final_fit, test))))
```
```{r 4.Random Forest 2, echo=FALSE, message=FALSE, warning=FALSE}
# Random Forest Model is very computationally expensive, so we only used 5000 observations of training set
final_fit <- ranger(medianHouseValue ~ longitude + latitude + housingMedianAge + population + avg_bedrooms + avg_rooms + medianIncome, data = train[1:5000,])
print(paste0("The RMSE of Random Forest model on test data is: ",rmse(test$medianHouseValue, predict(final_fit, test)$predictions)))
```
```{r 4.Gradient Boosted Tree 1, echo=FALSE, message=FALSE, warning=FALSE}
# loading library gbm
library(gbm)
```
```{r 4.Gradient Boosted Tree 2, echo=FALSE, message=FALSE, warning=FALSE}
# defining a grid that has different hyper parameters of the Gradient Boosted Tree
hyper_grid <- expand.grid(
  shrinkage = c(.01, .05),
  interaction.depth = c(3, 5),
  n.minobsinnode = c(5, 7),
  bag.fraction = c(.65, 1), 
  optimal_trees = 0,               # a place to dump results
  RMSE = 0                     # a place to dump results
)
# using k-fold (k=5) cross-validation on training data to find the best hyper parameters of the Gradient Boosted Tree
for(i in 1:nrow(hyper_grid)) {
  for (fold in folds) {
    gbm.tune <- gbm(
    formula = medianHouseValue ~ longitude + latitude + housingMedianAge + population + avg_bedrooms + avg_rooms + medianIncome,
    distribution = "gaussian",
    data = train[fold,],
    n.trees = 5000,
    interaction.depth = hyper_grid$interaction.depth[i],
    shrinkage = hyper_grid$shrinkage[i],
    n.minobsinnode = hyper_grid$n.minobsinnode[i],
    bag.fraction = hyper_grid$bag.fraction[i],
    train.fraction = .75,
    n.cores = NULL, # will use all cores by default
    verbose = FALSE
  )
    cv_mtry <- c(cv_mtry, 
                 rmse(train[-fold, "medianHouseValue"], predict(gbm.tune, train[-fold, ])))
  }
  hyper_grid$RMSE=mean(cv_mtry)
}
#hyper_grid$RMSE
```
```{r 4.Gradient Boosted Tree 3, echo=FALSE, message=FALSE, warning=FALSE}
# finding the best hyper parameter (that gave the least RMSE)
best_hyperparameter <- which.min(hyper_grid$RMSE)
# checking RMSE of the Gradient Boosted Tree model with the best hyper parameter on test data
final_fit <- gbm(
    formula = medianHouseValue ~ longitude + latitude + housingMedianAge + population + avg_bedrooms + avg_rooms + medianIncome,
    distribution = "gaussian",
    data = train,
    n.trees = 5000,
    interaction.depth = hyper_grid$interaction.depth[best_hyperparameter],
    shrinkage = hyper_grid$shrinkage[best_hyperparameter],
    n.minobsinnode = hyper_grid$n.minobsinnode[best_hyperparameter],
    bag.fraction = hyper_grid$bag.fraction[best_hyperparameter],
    train.fraction = .75,
    n.cores = NULL, # will use all cores by default
    verbose = FALSE
  )
print(paste0("The RMSE of Gradient Boosted Tree model on test data is: ",rmse(test$medianHouseValue, predict(final_fit, test))))
```
We can see that the Gradient Boosted Tree gives the least RMSE.  \
We will now use this to make the respective plots. We obtain the following geometric polygons:-    
```{r 4.loading librries for maps, echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
library(ggmap)
library(maps)
library(mapdata)
```
1) log medianHouseValue V/S longitude (x) and latitude (y)  
```{r log medianHouseValue V/S longitude (x) and latitude (y), echo=FALSE, message=FALSE, warning=FALSE}
mi_counties <- map_data("county", "california") %>% 
  select(lon = long, lat, group, id = subregion)
#head(mi_counties)
ggplot(mi_counties)+ geom_polygon(data = CAhousing, aes (x = longitude, y = latitude, color=log(medianHouseValue)))+coord_quickmap()+coord_fixed(1.3)
```
2) log predicted_medianHouseValue V/S longitude (x) and latitude (y)  
```{r log predicted_medianHouseValue V/S longitude (x) and latitude (y), echo=FALSE, message=FALSE, warning=FALSE}
predicted_medianHouseValue=predict(final_fit, CAhousing)
#print(predicted_median_price)
ggplot(mi_counties)+ geom_polygon(data = CAhousing, aes (x = longitude, y = latitude, color=log(predicted_medianHouseValue)))+coord_quickmap()+coord_fixed(1.3)
```
3) log AbsoluteResidual_medianHouseValue V/S longitude (x) and latitude (y)  
```{r log AbsoluteResidual_medianHouseValue V/S longitude (x) and latitude (y), echo=FALSE, message=FALSE, warning=FALSE}
AbsoluteResidual_medianHouseValue=abs(CAhousing$medianHouseValue - predicted_medianHouseValue)
ggplot(mi_counties)+ geom_polygon(data = CAhousing, mapping=aes (x = longitude, y = latitude, color=log(AbsoluteResidual_medianHouseValue)))+coord_quickmap()+coord_fixed(1.3)
```
